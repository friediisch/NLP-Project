{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%qtconsole\n",
    "import PyPDF2\n",
    "import re\n",
    "#import textract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PdfReadWarning: Xref table not zero-indexed. ID numbers for objects will be corrected. [pdf.py:1736]\n"
     ]
    }
   ],
   "source": [
    "#write a for-loop to open many files -- leave a comment if you'd #like to learn how\n",
    "filename = 'data/LREC/LREC2012_Proceedings/pdf/106_Paper.pdf'\n",
    "#open allows you to read the file\n",
    "pdfFileObj = open(filename,'rb')\n",
    "#The pdfReader variable is a readable object that will be parsed\n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "#discerning the number of pages will allow us to parse through all #the pages\n",
    "num_pages = pdfReader.numPages\n",
    "text = \"\"\n",
    "#The for loop will read each page\n",
    "for page_index in range(num_pages):\n",
    "    pageObj = pdfReader.getPage(page_index)\n",
    "    text += pageObj.extractText()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keyword Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['multimodal corpora', ' conversational video data', ' interaction engagement']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "startindex1 = text.find('\\n   Keywords:\\n ') + len('\\n   Keywords:\\n ')\n",
    "stopindex1 = text.find(' \\n 1. Introduction \\n')\n",
    "text[startindex1:stopindex1].split(',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Body Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = text.replace('\\n', '')\n",
    "startindex2 = text.find(' \\n 1. Introduction \\n')\n",
    "stopindex2 = re.search('\\s*\\n*\\d*\\.*\\s(References|\\w* References)\\s+\\n*', text, re.IGNORECASE).start()\n",
    "body = text[startindex2:stopindex2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "import sys\n",
    "\n",
    "path = \".\\\\data\\ACL\\\\\"\n",
    "if 'Apple' in sys.version:\n",
    "    path = \"data/ACL/\"\n",
    "url = 'http://aclweb.org/anthology/'\n",
    "html = urlopen(url).read()\n",
    "namecount = 1\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "L = ['J,', 'Q,', 'P,', 'E,', 'N,', 'D,', 'K,', 'S,', 'W,', 'A,', 'C,', 'H,', 'L,', 'Y,', 'O,', 'T']\n",
    "\n",
    "\n",
    "\n",
    "pattern_old = r'http:\\/\\/aclweb\\.org\\/anthology\\/[JQPENDKSWACHLYOT]\\/[JQPENDKSWACHLYOT]\\d\\d\\/'\n",
    "pattern = r'[JQPENDKSWACHLYTUX]\\/[JQPENDKSWACHLYTUX]\\d\\d\\/'\n",
    "tags = soup.findAll('a', href = re.compile(pattern))\n",
    "links = []\n",
    "for tag in tags:\n",
    "    #print(url + tag.attrs['href'])\n",
    "    links.append(url + tag.attrs['href'])\n",
    "\n",
    "\n",
    "def download_file(download_url):\n",
    "    global namecount\n",
    "    response = urlopen(download_url)\n",
    "    file = open('tmp.pdf', 'r+b')\n",
    "    file.write(response.read())\n",
    "    file.close()\n",
    "    pdfFileObj = open('tmp.pdf', 'rb')\n",
    "    pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "    num_pages = pdfReader.numPages\n",
    "    text = \"\"\n",
    "    #The for loop will read each page\n",
    "    for page_index in range(num_pages):\n",
    "        pageObj = pdfReader.getPage(page_index)\n",
    "        text += pageObj.extractText()\n",
    "    #with open(path + str(namecount) + '.txt', 'w') as f:\n",
    "    with open(str(namecount) + '.txt', 'w') as f:\n",
    "        namecount += 1\n",
    "        f.write(text)\n",
    "        \n",
    "\n",
    "def get_pdf(links, baseURL=None):\n",
    "    count = 0\n",
    "        \n",
    "    pdf_papers = []\n",
    "    for link in links:\n",
    "        if baseURL is None:\n",
    "            baseURL = link\n",
    "        html = urlopen(link).read()\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        pattern = r'\\.pdf$'\n",
    "        tags = soup.findAll('a', href = re.compile(pattern))\n",
    "        \n",
    "        for tag in tags:\n",
    "            count += 1\n",
    "            #print(baseURL.replace('.html', '/') + tag.attrs['href'])\n",
    "            pdf_papers.append(baseURL + tag.attrs['href'])\n",
    "            try:\n",
    "                download_file(baseURL.replace('.html', '/') + tag.attrs['href'])\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    return pdf_papers, count\n",
    "  \n",
    "\n",
    "pdfs, count1 = get_pdf(links)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "endings = ['ANN', 'BIOMED', 'DAT', 'DIAL', 'FSM', 'GEN', 'HAN', 'HUM', 'LEX', 'MEDIA', 'MOL', 'MT', 'NLL', 'PARSE', 'MORPHON', 'SEM', 'SLAV', 'SEMITIC', 'SLPAT', 'UR', 'WAC']\n",
    "endings = ['sig' + item.lower() for item in endings]\n",
    "\n",
    "links = []\n",
    "for ending in endings:\n",
    "    link = url + ending + '.html'\n",
    "    links.append(link)\n",
    "pdfs, count2 = get_pdf(links, url)\n",
    "\n",
    "\n",
    "print('number of pdf files: ', count1, count2, count1 + count2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code below is for LREC scraping, dont use fpr ACL! Similar to ExtractHTML.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "searching data/LREC/LREC2012_Proceedings/\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import bs4\n",
    "import PyPDF2\n",
    "import os\n",
    "\n",
    "years = [2010, 2012, 2014, 2016, 2018]\n",
    "years = [2012]\n",
    "datalist = []\n",
    "for year in years:\n",
    "    LRECdir = 'data/LREC/LREC' + str(year) + '_Proceedings/'\n",
    "    print('searching ' + str(LRECdir))\n",
    "    for summary in os.listdir(LRECdir + 'summaries/')[:10]:\n",
    "        \n",
    "        data = {}\n",
    "        \n",
    "        # open file\n",
    "        f=codecs.open(str(LRECdir + 'summaries/') + str(summary), 'r')\n",
    "        try:\n",
    "            soup = bs4.BeautifulSoup(f, 'html.parser')\n",
    "        except:\n",
    "            print(summary + ' not included')\n",
    "        \n",
    "        # get year\n",
    "        data['year'] = year\n",
    "        \n",
    "        # get number\n",
    "        data['number'] = summary.strip('.html')\n",
    "        \n",
    "        # extract title\n",
    "        title = soup.find('th', class_=\"second_summaries\").string\n",
    "        data['title'] = title\n",
    "\n",
    "        # extract topics\n",
    "        topics = []\n",
    "        for a in bs4.BeautifulSoup(str(soup.find_all(class_='topics_summaries')), 'html.parser').find_all('a'):\n",
    "            topics.append(a.string)\n",
    "        data['topics'] = topics\n",
    "        \n",
    "        # extract authors\n",
    "        authors = []\n",
    "        for a in soup.find('td', text = 'Authors').nextSibling.nextSibling.find_all('a'):\n",
    "            authors.append(a.string)\n",
    "        data['authors'] = authors\n",
    "        \n",
    "        # extract abstracts\n",
    "        data['abstract'] = soup.find('td', text = 'Abstract').nextSibling.nextSibling.string\n",
    "        datalist.append(data)\n",
    "        \n",
    "        # extract text body\n",
    "        if year == 2018:\n",
    "            pdfdir = LRECdir + 'pdf/' + str(data['number']) + '.pdf'\n",
    "        else:\n",
    "            pdfdir = LRECdir + 'pdf/' + str(data['number']) + '_Paper.pdf'\n",
    "        pdfFileObj = open(pdfdir, 'rb')\n",
    "        pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "        num_pages = pdfReader.numPages\n",
    "        text = \"\"\n",
    "        for page_index in range(num_pages):\n",
    "            pageObj = pdfReader.getPage(page_index)\n",
    "            text += pageObj.extractText()\n",
    "        data['fulltext'] = text\n",
    "        \n",
    "        # extract keywords\n",
    "            \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "#alltopics = []\n",
    "#for topiclist in topicsdict:\n",
    "#    for topic in topiclist.split(' '):\n",
    "#        alltopics.append(topic)\n",
    "#alltopics = list(set(alltopics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'PDFObjectNotFound'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-705dc82c5a75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpdfminer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpdfparser\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPDFParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpdfminer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpdfdocument\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPDFDocument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpdfminer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpdfpage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPDFPage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpdfminer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpdfpage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPDFTextExtractionNotAllowed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/studyenv/lib/python3.6/site-packages/pdfminer/pdfdocument.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpdftypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPDFTypeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpdftypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPDFStream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpdftypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPDFObjectNotFound\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpdftypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdecipher_all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpdftypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mint_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'PDFObjectNotFound'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    " \n",
    "import sys\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfpage import PDFTextExtractionNotAllowed\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from StringIO import StringIO\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.converter import TextConverter\n",
    " \n",
    "class MyParser(object):\n",
    "    def __init__(self, pdf):\n",
    "        ## Snipped adapted from Yusuke Shinyamas \n",
    "        #PDFMiner documentation\n",
    "        # Create the document model from the file\n",
    "        parser = PDFParser(open(pdf, 'rb'))\n",
    "        document = PDFDocument(parser)\n",
    "        # Try to parse the document\n",
    "        if not document.is_extractable:\n",
    "            raise PDFTextExtractionNotAllowed\n",
    "        # Create a PDF resource manager object \n",
    "        # that stores shared resources.\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "        # Create a buffer for the parsed text\n",
    "        retstr = StringIO()\n",
    "        # Spacing parameters for parsing\n",
    "        laparams = LAParams()\n",
    "        codec = 'utf-8'\n",
    " \n",
    "        # Create a PDF device object\n",
    "        device = TextConverter(rsrcmgr, retstr, \n",
    "                               codec = codec, \n",
    "                               laparams = laparams)\n",
    "        # Create a PDF interpreter object\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "        # Process each page contained in the document.\n",
    "        for page in PDFPage.create_pages(document):\n",
    "            interpreter.process_page(page)\n",
    "         \n",
    "        self.records            = []\n",
    "         \n",
    "        lines = retstr.getvalue().splitlines()\n",
    "        for line in lines:\n",
    "            self.handle_line(line)\n",
    "     \n",
    "    def handle_line(self, line):\n",
    "        # Customize your line-by-line parser here\n",
    "        self.records.append(line)\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    p = MyParser(sys.argv[1])\n",
    "    print('\\n'.join(p.records))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
