{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%qtconsole\n",
    "import PyPDF2\n",
    "import re\n",
    "#import textract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write a for-loop to open many files -- leave a comment if you'd #like to learn how\n",
    "filename = 'data/LREC/LREC2012_Proceedings/pdf/106_Paper.pdf'\n",
    "#open allows you to read the file\n",
    "pdfFileObj = open(filename,'rb')\n",
    "#The pdfReader variable is a readable object that will be parsed\n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "#discerning the number of pages will allow us to parse through all #the pages\n",
    "num_pages = pdfReader.numPages\n",
    "text = \"\"\n",
    "#The for loop will read each page\n",
    "for page_index in range(num_pages):\n",
    "    pageObj = pdfReader.getPage(page_index)\n",
    "    text += pageObj.extractText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['multimodal corpora', ' conversational video data', ' interaction engagement']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "startindex1 = text.find('\\n   Keywords:\\n ') + len(signal)\n",
    "stopindex1 = text.find(' \\n 1. Introduction \\n')\n",
    "text[startindex1:stopindex1].split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\n 1. Introduction \\nIn this paper we describe the Estonian corpus collection \\nand analysis activities and especially focus on the project \\n\\nMINT (Multimodal INTeraction), and its collection of \\n\\nthe Estonian First Encounters Dialogues. The aim of the \\n\\nMINT project is to create multimodal database which \\n\\nwill enable researchers to study interaction behaviours \\n\\nconcerning gesturing, synchrony and engagement in \\n\\nparticular, and also allow systematic comparison \\n\\nbetween Nordic and Baltic multimodal conversational \\n\\nstrategies. In this way, the project is connected to the \\n\\nwork in the Nordic context, where the MUMIN network \\n\\n(Allwood et al. 2007) and the ongoing NOMCO project \\n\\n(Paggio et al. 2010) have already paved the way to \\n\\ncollect and annotate comparable data that can be used to \\n\\ninvestigate communicative phenomena, especially \\n\\nfeedback, turn management and sequencing. The data \\n\\nwill make it possible to empirically study multimodal \\n\\nsignals (head movements, facial displays, hand gestures \\n\\nand body postures) and their relation to spoken \\n\\nutterances, and moreover, to compare and analyse similar \\n\\nphenomena and communicative strategies in different but \\n\\nneighbouring languages.  \\n\\nIn this paper we describe our corpus collection activities \\n\\nand especially introduce the recent collection of the \\n\\nEstonian First Encounters data. We also discuss some \\n\\nissues related to conversational engagement and \\n\\ncooperation, and provide observations on the \\n\\nparticipants™ views concerning the interaction they have \\n\\nbeen engaged. \\n2. The Corpus Collection Projects  \\nAt the University of Tartu we have two research groups \\n\\nworking on multimodal communication and the analysis \\n\\nof conversational video data. The groups have different \\n\\nfocus points but enjoy synergy built on mutual \\n\\ncooperation. \\n\\nWithin the project MINT (\\nMultimodal Interaction Œ \\nintercultural and technological aspects of video data \\ncollection, analysis, and use)\\n we have collected a corpus \\nof Estonian First Encounter dialogues. The project is \\n\\nfunded by the Estonian National Science Foundation \\n\\n(ETF), and it focuses on the interlocutors™ multimodal \\n\\nmeans and strategies for building shared understanding, \\n\\non the basis of their participation in a particular activity. \\n\\nImportant aspects deal with intercultural comparisons \\n\\nconcerning the participants™ engagement and synchrony \\n\\nin various communicative activities, and building models \\n\\nfor their automatic processing. As human-machine \\n\\ninteractions get more complex, such models are crucial \\n\\nin designing and constructing different applications, e.g. \\n\\ninteractive robot agents (Wilcock and Jokinen, 2011). \\n\\nThe goals of the MINT project are:  \\n\\na) to create Estonian multimodal video corpus on various \\n\\nconversational activities, \\n\\nb) to provide analysis and annotation of the data that \\n\\ncontributes to the previous work on annotation standards, \\n\\nguidelines, and schemes, \\n\\nc) to study multimodal signals, especially gesturing, in \\n\\nsocial communication and conversation management, \\n\\nand indicating the interlocutors™ engagement and \\n\\nsynchrony in communicative activity, \\n\\nd) to build (computational) models for the coordination \\n\\nand controlling of interaction (e.g. taking turns, giving \\n\\nfeedback), and constructing shared understanding, and \\n\\nf) to investigate techniques and means for automatic \\n\\nrecognition of multimodal signals, especially gestures. \\n\\n \\nThis MINT project is supported by the multimodal \\n\\ncommunication research group (MUSU), and another \\n\\nETF project \\nThe structure of multimodal communication \\nand the choice of communication strategies\\n. This project \\naims to identify the communicative behaviours important \\n\\nfrom the perspective of a particular communicative \\n\\nsituation in social interaction, and to analyse the choice \\n\\nand use of means of communication within this context. \\n\\nThe MUSU group is currently compiling a multimodal \\n\\ncommunication database, \\nthe Multimodal \\nCommunication Research Corpus. The corpus has two \\n\\nsub-corpora: corpus of interactive communicative \\n2764situations (ISU) and corpus of contextualized written \\ntexts (KOK). The ISU sub-corpus contains data in the \\n\\nform of video and audio recording from real-life \\n\\nEstonian communicative settings, e.g. multiparty casual \\n\\nconversations in the borderland of East-Estonia, \\n\\nsituations in language learning classroom, and specific \\n\\nmaterial such as communication situations with Patau \\n\\nsyndrome subject, etc. The KOK sub-corpus contains \\n\\ndata in the form of written texts, e.g. literal translations, \\n\\nhistorical manuscripts, etc.  \\n3. First Encounters Data Collection \\nIn the first phase, the MINT project has collected a \\n\\ncorpus of first encounters following the guidelines of the \\n\\nproject NOMCO. The first encounter dialogues engage \\n\\nparticipants, who do not know each other in advance, in \\n\\nan activity where their task is to chat and make \\n\\nacquaintance with each other. Original data was collected \\n\\nin the Estonian language, and the data is annotated and \\n\\nanalysed using an annotation scheme which is co-\\n\\nmeasurable with the annotations used in NOMCO.  \\n\\nEach participant was given a short presentation of the \\n\\nproject and the goals of the data collection before the \\n\\nrecording, and they were also asked to sign a consent \\n\\nform (in the Estonian language) that grants permission \\n\\nfor their video data to be used for research purposes, and \\n\\nto be shown to third parties without further permission. \\n\\nThe collection setup is shown in Figure 1. The \\n\\nparticipants entered the recording environment through \\n\\nthe doors at both ends of the video setup room, and if \\n\\nthey arrived too early and needed to wait for their time, it \\n\\nwas made sure that the pairs did not see each but in the \\n\\nexperiment room. They were asked to proceed to the line \\n\\nmarked on the floor. This was to ensure that both \\n\\nparticipants were approximately in the middle of the \\n\\nvideo camera views. \\n\\nThree cameras were used: one recording each of the two \\n\\npartners (marked by yellow and green balls in Figure 1, \\n\\nand one recording both (marked red in Figure 1). We used \\n\\nSonyHDR-XR550V cameras with three external Sony \\n\\nECM-HW2 wireless microphones. The microphones were \\n\\npaired with cameras so that each camera had its own \\n\\naudio track. As for the video recording, we chose the full \\nHD quality mode, although it turned out that the standard \\n\\nquality would have been good enough.  \\n\\nThe camera views were cut, edit and merged via Sony \\n\\nVegas Pro 11, and they were syncronised and integrated \\n\\ninto one single video film providing a mosaic view of the \\n\\nsituation (as in the similar Finnish encounters). This is \\n\\nshown in Figure 2.  \\n\\nWe have a total of 23 participants (12 male and 11 \\n\\nfemale), with age ranging between 21 and 61 years. The \\n\\nparticipants are native speakers of Estonian and they are \\n\\nstudents or university employees. Each participant took \\n\\npart in two encounters, i.e. with two different partners. \\n\\nThe corpus contains 23 encounters, and each encounter \\n\\nis about 8 minutes long. They balanced with gender \\n\\ndistribution and we have 8 female-female encounters, 7 \\n\\nfemale-male encounters, and 8 male-male encounters. \\n4. Participants™ Views of the Interaction \\nWe also conducted a small questionnaire regarding the \\n\\nparticipants™ impressions and feelings of the interaction. \\n\\nThe questionnaire was in a web format and the \\n\\nparticipants answered the questions immediately after \\n\\neach interaction. The questionnaire was in Estonian, and \\n\\nasked if the participants considered the interaction \\n\\nenjoyable, friendly, impressive, nice, interesting, relaxed, \\n\\nanxious, natural, happy, tense, awkward, angry in a 5-\\n\\npoint Likert scale, where 5 indicates agreement and 1 \\n\\ndisagreement with the adjective in question.  \\n\\nThe average ratings among the 23 participants are given in \\n\\nTable 1 (next page). As can be seen, the participants have \\n\\nhad rather positive experience of the interactions, with the \\n\\ntop impression (4.2/5) being happy. Also the adjectives \\n\\nenjoyable, nice, and interesting are found appropriate in \\n\\ndescribing interaction experience, while the participants \\n\\ndid not regard their interactions as being angry at all, and, \\n\\nsomewhat surprisingly, they didn™t seem to consider the \\n\\ninteractions very awkward, tense, or anxious either. \\n\\nSince we also have demographic information about the \\n\\nparticipants™ gender, age group, education, self-estimated \\n\\nknowledge of the computers and self-estimated familiarity \\n\\nwith videos, we did some detailed studies to check if there \\n\\nare differences between the participants™ experience along \\nFigure 1 The data collection setup\\n Figure 2 A mosaic view of the video recordings.\\n 27650,0\\n1,0\\n2,0\\n3,0\\n\\n4,0\\n\\n5,0\\nmale\\nfemale\\n0,0\\n1,0\\n\\n2,0\\n\\n3,0\\n\\n4,0\\n\\n5,0\\nbeginner\\nmiddle\\nadvanced\\nthese factors. Using Student™s t-test, we first compared the \\nevaluation values with respect to gender.  \\nTable 1 Participants™ average impressions of the interaction. \\nThe values are given in a 5-point Likert scale with 5 \\nindicating agreement and 1 disagreement. \\nIt turned out that differences between the average male \\nand female participants are not statistically significant \\n\\nexcept for the value ﬁinterestingﬂ: the male participants \\n\\nconsider their interactions more interesting than female \\n\\nparticipants (means: 4.4 vs. 3.6) at p < 0.01 (t= 2.68, \\n\\ndegrees of freedom 44, standard deviation 0.775). In \\n\\ngeneral, male participants also considered the dialogues \\n\\nmore friendly and impressive, but also more anxious and \\n\\ntense than females, while female participants considered \\n\\nthe dialogues slightly more enjoyable and natural. \\n\\nConcerning age groups, 3/4 of the participants are \\n\\nbetween 20-30 years of age, so there was not enough data \\n\\nto find significant differences between the age groups. \\n\\nHowever, on average, the 20-30 years old considered their \\ninteractions slightly nicer and more relaxed than did those \\n\\nover 30 years of age, while the latter found their \\n\\ninteractions more enjoyable, friendly and interesting.  \\n\\n As for differences in regard to education levels (graduate \\n\\nstudents vs. those who study on the Master™s level having \\n\\ncompleted their bachelor degree vs. those who had \\n\\ncompleted Master™s degree), they were not significant \\n\\neither. However, some divergence was obvious on \\n\\nindividual aspects and e.g. graduate students regarded \\n\\ntheir interactions less relaxed and more anxious, yet more \\n\\ninteresting than those who had bachelor degree. Those \\n\\nwith master™s degree were in general more positive than \\n\\nthe others, but as said the differences were not significant. \\n\\nNeither were deviations with respect to computer \\n\\nknowledge significant. Participants who estimated \\n\\nthemselves experts seem to find interactions slightly more \\n\\nenjoyable, friendly, interesting, natural, and happy than \\n\\nthose who estimated themselves as advanced users, while \\n\\nthe latter tend to rate the interactions more awkward and \\n\\nanxious than the experts. It is interesting that none of the \\n\\nparticipants self-estimated themselves as ﬁordinaryﬂ users, \\n\\ni.e. IT knowledge was generally regarded as common \\n\\nrather than a special skill. \\n\\nHowever, an intriguing result is that the participants™ self-\\n\\nestimated familiarity with video recordings, video analysis \\n\\nand videos in general (beginner, middle, advanced) caused \\n\\nstatistically significant differences in their experience. In \\n\\nparticular, those who regarded themselves as having \\n\\nadvanced familiarity with the videos found interactions \\n\\nmore interesting (mean: 4.5) than those who regarded \\n\\nthemselves as beginners (mean: 3.7) at p > 0.05, t=2.37, \\n\\ndegrees of freedom 18, standard deviation 0.770. Those \\n\\nwith advanced familiarity also contrasted with participant \\n\\nwho considered themselves in the middle, in that the \\n\\ninteractions were happy (mean: 4.4 vs. 3.6) at p < 0.05 \\ndescriptive \\nfeature \\naverage\\n min\\n max enjoyable\\n 4.1 1 5 friendly\\n 3.0 1 5 impressive\\n 3.7 1 5 nice\\n 4.1 2 5 interesting\\n 4.1 2 5 relaxed\\n 3.6 1 5 anxious\\n 2.3 1 5 natural\\n 3.4 1 5 happy 4.2 2 5 tense\\n 2.0 1 4 awkward\\n 1.9 1 5 angry 1.0 1 2 Average\\n 3.1   Figure 3 No significant difference between gender impressions except for interesting (p < 0.01) \\nFigure 4 Significant differences between advanced and beginners on interesting, and betwe\\nen advanced and middle on \\nhappy (p < 0.05).\\n 2766(t=2.27, degrees of freedom 32, standard deviation 0.786).  \\nBig deviations were also found in that the advanced \\n\\nparticipants found interactions more awkward, anxious, \\n\\nand tense than the middle-level participants, and they \\n\\nconsidered interaction more tense, yet nicer than the \\n\\nbeginners.  \\n\\nAs said, not all differences are significant. However, what \\n\\nis significant is the contrast between participants who \\n\\nevaluate themselves as experts vs. not experts on videos \\n\\nand video recordings. Apparently the participants™ interest \\n\\nand knowledge of the novel technology also carries over \\n\\nto situations where the novel technology is used, and thus \\n\\nalso affects their experience of the communication in that \\n\\nparticular situation. It is good to remember that the \\n\\nparticipants™ knowledge of computers and information \\n\\ntechnology as such did not cause a similar effect in the \\n\\nparticipant™s experience. This supports the hypothesis that \\n\\nnew expertise or knowledge which deviates from the \\n\\nusual knowledge in conversational situations tends to have \\n\\na positive effect on the evaluation and experience of the \\n\\nsituation as a whole. \\n5. Engagement \\nConversations are cooperative activities through which \\n\\nthe participants aim at achieving some underlying goals \\n\\nof the interaction. The goals can range from specific \\n\\ntask-related goals (e.g. to instruct someone to use \\n\\nannotation software, to provide information about bus \\n\\ntime, to learn to know each other) to an intention just to \\n\\nkeep the ﬁchannel openﬂ.  The interlocutors react to each \\n\\nothers™ actions and coordinate their turns in a manner \\n\\nthat allows both to present their message in a cooperative \\n\\nmanner. Engagement is an important sign of this kind of \\n\\ncooperation, and it is related to the interlocutors™ \\n\\nexperience of the interaction in general: more engaged \\n\\nthe interlocutors are in the conversation, the more \\n\\npositively they may experience the interaction. \\n\\nOur interest in studying conversational engagement goes \\n\\nback to intelligent systems and interaction technology, \\n\\nwhere engagement is used to describe the user™s \\n\\nwillingness and involvement in the interaction with the \\n\\nautomatic interactive system. If it is possible to measure \\n\\nthe interlocutors™ engagement level, it is easier to adjust \\n\\nthe system™s conversational strategies accordingly.  \\n\\nAn intuitive definition of engagement is that it refers to \\n\\nthe situation where the participants are involved in a \\n\\nconversation and show basic willingness to listen to the \\n\\npartner and provide coherent contributions. However, the \\n\\nmore active the interlocutors become, the more engaged \\n\\nthey seem to be in the conversation: their speaking \\n\\nfrequency, tone of voice and body posture indicate \\n\\ninterest and commitment to the topic of the conversation: \\n\\nthey engagement becomes embodied. Usually such \\n\\nactivity is reinforced by the partner™s actions on a similar \\n\\nlevel of engagement, so we can talk about mutual \\n\\nengagement. Such interactions are described as pleasant, \\n\\ninspiring, and fun.  \\n\\nWe will investigate various action patterns and \\n\\nbehaviours that are typical for the participants when they \\nare engaged in interaction they describe as nice, \\n\\nenjoyable, interesting, natural, pleasant.  We will explore \\n\\nsome measures of engagement in terms of the \\n\\ninterlocutors™ verbal and non-verbal communicative \\n\\nactivity, in particular their gesture activity and study if \\n\\nengagement can be operationalised through these signals. \\n\\nPrevious work has used such measures as frequency \\n\\ncounts and the utterance density (Campbell and Scherer, \\n\\n2010; Jokinen, 2011), and we will follow these lines as is \\n\\nappropriate.  \\n\\nWe also emphasise the close relationship between speech \\n\\nand multimodal information in the processing of human \\n\\nconversational interactions. The semantic content of \\n\\nlinguistic utterances is accompanied by hand gestures, \\n\\nbody movement, eye-gaze, and non-speech vocalisations \\n\\nwhich are used as tacit signals to indicate the speaker's \\n\\nfocus of attention, intentions, emphasis, emotional state, \\n\\netc. They should, of course, be processed alongside the \\n\\nlanguage expressions.  \\n\\nMoreover, the participants™ synchrony with each other is \\n\\nregarded as one of the pertinent signs of cooperation: the \\n\\ninterlocutors intuitively tend to follow the partner™s \\n\\ncommunication and produce similar behaviour, thus \\n\\ncontributing to the construction of the shared context and \\n\\nmutual understanding. This kind of adaptation to each \\n\\nother™s behaviour is often called alignment (Pickering \\n\\nand Garrod, 2004), or mimicry, and it can take place \\n\\nverbally (words, prosody) or non-verbally (gestures, \\n\\nbody posture). It is thus an important sign of the partners™ \\n\\nengagement in the interaction, and our previous works \\n\\non this can be found in Jokinen and Pärkson (2011) as \\n\\nwell as in Rummo and Tenjes (2011).  \\n\\nWe also have data and first results about specific type of \\n\\ncommunication Œ subject with the Patau syndrome. The \\n\\npreliminary results of the analysis of data (see Jokinen et \\n\\nal. forthcoming) reveal meaningful nonverbal behaviour \\n\\nthrough touching. For instance, the Patau subject put her \\n\\nhand on her partner™s shoulder, creating her own \\n\\ncommunicative space. From this act we can conclude \\n\\nthat \\ntouch\\n is clearly one component of Patau subject™s \\nlanguage. This type of nonverbal interactive behaviour \\n\\nguarantees to subject that her partner is involved in the \\n\\ninteraction as well as supplies her necessity of adjacency. \\n\\nWhen people engage in dialogue, they use verbal and \\n\\nnon-verbal cues to structure the conversation flow and \\n\\nprovide feedback about the current understanding of the \\n\\ndiscourse. An intuitive measure of cooperation between \\n\\ninterlocutors is their verbal and non-verbal \\n\\ncommunicative activity, and we hypothesize that \\n\\nengagement can be measured analogously, with respect \\n\\nto their cooperation. It can be estimated by measuring the \\n\\nparticipants™ verbal and non-verbal activity. In this we \\n\\nuse both quantitative and qualitative analysis; the latter \\n\\ngives an opportunity to show variety of the research \\n\\nmaterial. \\n\\nIn another previous study, we used annotated multiparty \\n\\nconversations where three participants (who are familiar \\n\\nwith each other) had been assigned certain roles related \\n\\nto a simulated school inspection situation. The analysis \\n2767shows that participants use a full repertoire of different \\nnon-verbal signals as signs of their engagement in the \\n\\nconversation: hand gestures, facial displays, nods, and \\n\\nbody movement. For instance, the speakers animate and \\n\\nemphasise their speech by hand gestures, so as to give \\n\\nimportance to a particular part of their speech, and they \\n\\nalso use gestures to control and coordinate conversation \\n\\nflow. Also the speakers™ gaze is used as a pointing \\n\\ngesture: it can mark the speaker who is expected to take \\n\\nthe next turn (mutual gaze). It is also related to the \\n\\nprocessing of the given information and is effectively \\n\\nused to create social bonds between the interlocutors. \\n\\nSome body movements are also used to fill pauses in \\n\\nconversation: if the speaker does not want to take the \\n\\nturn or is unable to take the turn, they usually withdraw \\n\\nfrom the centre of the conversational space. By body \\n\\nmovement, the participants tacitly indicate that they are \\n\\npresent in the conversation.  \\n\\nOther communicative signals include nodding and \\n\\nlaughing. Nodding is related to the interlocutor™s \\n\\nengagement because the very act of nodding signals that \\n\\nthe person takes part in a conversation and is ready for \\n\\ncooperation. Laughing often occurs in smooth \\n\\nconversations at the same time. On other hand, laughing \\n\\ncan also illustrates the case where engagement is \\n\\nunevenly distributed among the interlocutors: not all are \\n\\nengaged to the same level at the same time. \\n\\nThe fresh First Encounters corpus will provide valuable \\n\\nreference material for these earlier corpora and related \\n\\nstudies. Moreover, it may help us also to shed light on \\n\\nthe general question of whether the participants™ \\n\\nnonverbal behaviour differs in role-playing and \\n\\nspontaneous situations, and if so what are the \\n\\ncharacteristics.  \\n6. Conclusions and Future Work \\nSeveral interesting issues emerged from the corpus \\n\\ncollection and preliminary data analyses, which can lead \\n\\nto subsequent research. For instance, we have studied the \\n\\ninterlocutors™ self-evaluation of the interaction they were \\n\\nengaged in, and although the dataset is relatively small, \\n\\nwe found statistically significant differences concerning \\n\\nhow the participants experienced the interaction. \\n\\nInterestingly, the participants™ expertise on the novel \\n\\ntechnology that was used in the data collection seems to \\n\\nbe one of the distinguishing factors, indicating that \\n\\nengagement and experience are complex issues, related \\n\\nto intrigue aspects of individual skills and knowledge.  \\n\\nWe will also continue engagement studies and aim at \\n\\nbuilding models that would allow us to better understand \\n\\nthe complex process of cooperation and coordination of \\n\\ninteractions. For this, we will especially study gesturing \\n\\nand gesture recognition, and can use all the that has been \\n\\ncollected in various activities. \\n\\nThe research will also focus on intercultural comparison \\n\\nof the collected conversational data. Systematic \\n\\nmultimodal communication studies are useful, and they \\n\\ncan provide valuable empirical evidence for the \\n\\nobservations and views concerning different \\ncommunication styles and strategies in different cultures. \\n\\nEspecially in the Nordic context, comparisons between \\n\\nthe first encounter dialogues in the neighbouring countries \\n\\nwill be particularly interesting because the countries have \\n\\na long history of various interactions and encounters on \\n\\npolitical and cultural levels, while the languages are pair-\\n\\nwise linguistically related. It is also possible to continue \\n\\nthis kind of comparison by extending our research to \\n\\ncorpora that represent larger cultural differences, such as \\n\\nJapanese (Jokinen et al. 2010). \\n\\nWe are currently in the process of transcribing and \\n\\nannotating the First Encounters Data. As future work, we \\n\\nplan to collect more data, and may especially focus on \\n\\nmulti-party conversations where the participants have \\n\\ndifferent roles: the speaker, the main addressee, and the \\n\\nside addressee(s). In these situations the participants™ role \\n\\nis important and their level of engagement can differ \\n\\ndepending on whether the participant is actively engaged \\n\\nin the interaction or listening to a conversation as a side \\n\\nparticipant. For instance, the side participant need not \\n\\nreact at the same time as the main addressee, and still be \\n\\nengaged in the conversation. \\n7. Acknowledgements \\nWe would like to thank the participants who took part in \\nthe video recordings and Sven Laater who took care of \\n\\nthe practical video setup. We would also like to thank \\nMare Koit and her project in the Estonian Centre of \\nExcellence in Computer Science (EXCS) for \\n\\nencouragement and financial support for the work. The \\n\\nMINT project is financially supported by the Estonian \\n\\nScience Foundation grant ETF8958.\""
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#text = text.replace('\\n', '')\n",
    "startindex2 = text.find(' \\n 1. Introduction \\n')\n",
    "stopindex2 = re.search('\\s*\\n*\\d*\\.*\\s(References|\\w* References)\\s+\\n*', text, re.IGNORECASE).start()\n",
    "text[startindex2:stopindex2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.6.6 | packaged by conda-forge | (default, Jul 26 2018, 09:55:02) \\n[GCC 4.2.1 Compatible Apple LLVM 6.1.0 (clang-602.0.53)]'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Investigating Engagement \\nIntercultural and technological aspects of the collection, analysis, and use of \\nEstonian Conversational Video Data \\nKristiina Jokinen\\n1, Silvi Tenjes\\n2 University of Tartu \\n1Department of Computer Science, J.Liivi 2, Tartu \\n2Department of Estonian as a Foreign Language, Jakobi 2-432, Tartu \\nE-mail: \\nkristiina.jokinen@helsinki.fi\\n, silvi.tenjes@ut.ee\\n Abstract  \\nIn this paper we describe the goals of the Estonian corpus collection and analysis activities, and introduce the recent collection of \\nEstonian First Encounters data. The MINT project aims at deepening our understanding of the conversational properties and \\npractices in human interactions. We especially investigate conversational engagement and cooperation, and discuss some \\nobservations on the participants™ views concerning the interaction they have been engaged.\\n   Keywords:\\n multimodal corpora, conversational video data, interaction engagement \\n 1. Introduction \\nIn this paper we describe the Estonian corpus collection \\nand analysis activities and especially focus on the project \\n\\nMINT (Multimodal INTeraction), and its collection of \\n\\nthe Estonian First Encounters Dialogues. The aim of the \\n\\nMINT project is to create multimodal database which \\n\\nwill enable researchers to study interaction behaviours \\n\\nconcerning gesturing, synchrony and engagement in \\n\\nparticular, and also allow systematic comparison \\n\\nbetween Nordic and Baltic multimodal conversational \\n\\nstrategies. In this way, the project is connected to the \\n\\nwork in the Nordic context, where the MUMIN network \\n\\n(Allwood et al. 2007) and the ongoing NOMCO project \\n\\n(Paggio et al. 2010) have already paved the way to \\n\\ncollect and annotate comparable data that can be used to \\n\\ninvestigate communicative phenomena, especially \\n\\nfeedback, turn management and sequencing. The data \\n\\nwill make it possible to empirically study multimodal \\n\\nsignals (head movements, facial displays, hand gestures \\n\\nand body postures) and their relation to spoken \\n\\nutterances, and moreover, to compare and analyse similar \\n\\nphenomena and communicative strategies in different but \\n\\nneighbouring languages.  \\n\\nIn this paper we describe our corpus collection activities \\n\\nand especially introduce the recent collection of the \\n\\nEstonian First Encounters data. We also discuss some \\n\\nissues related to conversational engagement and \\n\\ncooperation, and provide observations on the \\n\\nparticipants™ views concerning the interaction they have \\n\\nbeen engaged. \\n2. The Corpus Collection Projects  \\nAt the University of Tartu we have two research groups \\n\\nworking on multimodal communication and the analysis \\n\\nof conversational video data. The groups have different \\n\\nfocus points but enjoy synergy built on mutual \\n\\ncooperation. \\n\\nWithin the project MINT (\\nMultimodal Interaction Œ \\nintercultural and technological aspects of video data \\ncollection, analysis, and use)\\n we have collected a corpus \\nof Estonian First Encounter dialogues. The project is \\n\\nfunded by the Estonian National Science Foundation \\n\\n(ETF), and it focuses on the interlocutors™ multimodal \\n\\nmeans and strategies for building shared understanding, \\n\\non the basis of their participation in a particular activity. \\n\\nImportant aspects deal with intercultural comparisons \\n\\nconcerning the participants™ engagement and synchrony \\n\\nin various communicative activities, and building models \\n\\nfor their automatic processing. As human-machine \\n\\ninteractions get more complex, such models are crucial \\n\\nin designing and constructing different applications, e.g. \\n\\ninteractive robot agents (Wilcock and Jokinen, 2011). \\n\\nThe goals of the MINT project are:  \\n\\na) to create Estonian multimodal video corpus on various \\n\\nconversational activities, \\n\\nb) to provide analysis and annotation of the data that \\n\\ncontributes to the previous work on annotation standards, \\n\\nguidelines, and schemes, \\n\\nc) to study multimodal signals, especially gesturing, in \\n\\nsocial communication and conversation management, \\n\\nand indicating the interlocutors™ engagement and \\n\\nsynchrony in communicative activity, \\n\\nd) to build (computational) models for the coordination \\n\\nand controlling of interaction (e.g. taking turns, giving \\n\\nfeedback), and constructing shared understanding, and \\n\\nf) to investigate techniques and means for automatic \\n\\nrecognition of multimodal signals, especially gestures. \\n\\n \\nThis MINT project is supported by the multimodal \\n\\ncommunication research group (MUSU), and another \\n\\nETF project \\nThe structure of multimodal communication \\nand the choice of communication strategies\\n. This project \\naims to identify the communicative behaviours important \\n\\nfrom the perspective of a particular communicative \\n\\nsituation in social interaction, and to analyse the choice \\n\\nand use of means of communication within this context. \\n\\nThe MUSU group is currently compiling a multimodal \\n\\ncommunication database, \\nthe Multimodal \\nCommunication Research Corpus. The corpus has two \\n\\nsub-corpora: corpus of interactive communicative \\n2764situations (ISU) and corpus of contextualized written \\ntexts (KOK). The ISU sub-corpus contains data in the \\n\\nform of video and audio recording from real-life \\n\\nEstonian communicative settings, e.g. multiparty casual \\n\\nconversations in the borderland of East-Estonia, \\n\\nsituations in language learning classroom, and specific \\n\\nmaterial such as communication situations with Patau \\n\\nsyndrome subject, etc. The KOK sub-corpus contains \\n\\ndata in the form of written texts, e.g. literal translations, \\n\\nhistorical manuscripts, etc.  \\n3. First Encounters Data Collection \\nIn the first phase, the MINT project has collected a \\n\\ncorpus of first encounters following the guidelines of the \\n\\nproject NOMCO. The first encounter dialogues engage \\n\\nparticipants, who do not know each other in advance, in \\n\\nan activity where their task is to chat and make \\n\\nacquaintance with each other. Original data was collected \\n\\nin the Estonian language, and the data is annotated and \\n\\nanalysed using an annotation scheme which is co-\\n\\nmeasurable with the annotations used in NOMCO.  \\n\\nEach participant was given a short presentation of the \\n\\nproject and the goals of the data collection before the \\n\\nrecording, and they were also asked to sign a consent \\n\\nform (in the Estonian language) that grants permission \\n\\nfor their video data to be used for research purposes, and \\n\\nto be shown to third parties without further permission. \\n\\nThe collection setup is shown in Figure 1. The \\n\\nparticipants entered the recording environment through \\n\\nthe doors at both ends of the video setup room, and if \\n\\nthey arrived too early and needed to wait for their time, it \\n\\nwas made sure that the pairs did not see each but in the \\n\\nexperiment room. They were asked to proceed to the line \\n\\nmarked on the floor. This was to ensure that both \\n\\nparticipants were approximately in the middle of the \\n\\nvideo camera views. \\n\\nThree cameras were used: one recording each of the two \\n\\npartners (marked by yellow and green balls in Figure 1, \\n\\nand one recording both (marked red in Figure 1). We used \\n\\nSonyHDR-XR550V cameras with three external Sony \\n\\nECM-HW2 wireless microphones. The microphones were \\n\\npaired with cameras so that each camera had its own \\n\\naudio track. As for the video recording, we chose the full \\nHD quality mode, although it turned out that the standard \\n\\nquality would have been good enough.  \\n\\nThe camera views were cut, edit and merged via Sony \\n\\nVegas Pro 11, and they were syncronised and integrated \\n\\ninto one single video film providing a mosaic view of the \\n\\nsituation (as in the similar Finnish encounters). This is \\n\\nshown in Figure 2.  \\n\\nWe have a total of 23 participants (12 male and 11 \\n\\nfemale), with age ranging between 21 and 61 years. The \\n\\nparticipants are native speakers of Estonian and they are \\n\\nstudents or university employees. Each participant took \\n\\npart in two encounters, i.e. with two different partners. \\n\\nThe corpus contains 23 encounters, and each encounter \\n\\nis about 8 minutes long. They balanced with gender \\n\\ndistribution and we have 8 female-female encounters, 7 \\n\\nfemale-male encounters, and 8 male-male encounters. \\n4. Participants™ Views of the Interaction \\nWe also conducted a small questionnaire regarding the \\n\\nparticipants™ impressions and feelings of the interaction. \\n\\nThe questionnaire was in a web format and the \\n\\nparticipants answered the questions immediately after \\n\\neach interaction. The questionnaire was in Estonian, and \\n\\nasked if the participants considered the interaction \\n\\nenjoyable, friendly, impressive, nice, interesting, relaxed, \\n\\nanxious, natural, happy, tense, awkward, angry in a 5-\\n\\npoint Likert scale, where 5 indicates agreement and 1 \\n\\ndisagreement with the adjective in question.  \\n\\nThe average ratings among the 23 participants are given in \\n\\nTable 1 (next page). As can be seen, the participants have \\n\\nhad rather positive experience of the interactions, with the \\n\\ntop impression (4.2/5) being happy. Also the adjectives \\n\\nenjoyable, nice, and interesting are found appropriate in \\n\\ndescribing interaction experience, while the participants \\n\\ndid not regard their interactions as being angry at all, and, \\n\\nsomewhat surprisingly, they didn™t seem to consider the \\n\\ninteractions very awkward, tense, or anxious either. \\n\\nSince we also have demographic information about the \\n\\nparticipants™ gender, age group, education, self-estimated \\n\\nknowledge of the computers and self-estimated familiarity \\n\\nwith videos, we did some detailed studies to check if there \\n\\nare differences between the participants™ experience along \\nFigure 1 The data collection setup\\n Figure 2 A mosaic view of the video recordings.\\n 27650,0\\n1,0\\n2,0\\n3,0\\n\\n4,0\\n\\n5,0\\nmale\\nfemale\\n0,0\\n1,0\\n\\n2,0\\n\\n3,0\\n\\n4,0\\n\\n5,0\\nbeginner\\nmiddle\\nadvanced\\nthese factors. Using Student™s t-test, we first compared the \\nevaluation values with respect to gender.  \\nTable 1 Participants™ average impressions of the interaction. \\nThe values are given in a 5-point Likert scale with 5 \\nindicating agreement and 1 disagreement. \\nIt turned out that differences between the average male \\nand female participants are not statistically significant \\n\\nexcept for the value ﬁinterestingﬂ: the male participants \\n\\nconsider their interactions more interesting than female \\n\\nparticipants (means: 4.4 vs. 3.6) at p < 0.01 (t= 2.68, \\n\\ndegrees of freedom 44, standard deviation 0.775). In \\n\\ngeneral, male participants also considered the dialogues \\n\\nmore friendly and impressive, but also more anxious and \\n\\ntense than females, while female participants considered \\n\\nthe dialogues slightly more enjoyable and natural. \\n\\nConcerning age groups, 3/4 of the participants are \\n\\nbetween 20-30 years of age, so there was not enough data \\n\\nto find significant differences between the age groups. \\n\\nHowever, on average, the 20-30 years old considered their \\ninteractions slightly nicer and more relaxed than did those \\n\\nover 30 years of age, while the latter found their \\n\\ninteractions more enjoyable, friendly and interesting.  \\n\\n As for differences in regard to education levels (graduate \\n\\nstudents vs. those who study on the Master™s level having \\n\\ncompleted their bachelor degree vs. those who had \\n\\ncompleted Master™s degree), they were not significant \\n\\neither. However, some divergence was obvious on \\n\\nindividual aspects and e.g. graduate students regarded \\n\\ntheir interactions less relaxed and more anxious, yet more \\n\\ninteresting than those who had bachelor degree. Those \\n\\nwith master™s degree were in general more positive than \\n\\nthe others, but as said the differences were not significant. \\n\\nNeither were deviations with respect to computer \\n\\nknowledge significant. Participants who estimated \\n\\nthemselves experts seem to find interactions slightly more \\n\\nenjoyable, friendly, interesting, natural, and happy than \\n\\nthose who estimated themselves as advanced users, while \\n\\nthe latter tend to rate the interactions more awkward and \\n\\nanxious than the experts. It is interesting that none of the \\n\\nparticipants self-estimated themselves as ﬁordinaryﬂ users, \\n\\ni.e. IT knowledge was generally regarded as common \\n\\nrather than a special skill. \\n\\nHowever, an intriguing result is that the participants™ self-\\n\\nestimated familiarity with video recordings, video analysis \\n\\nand videos in general (beginner, middle, advanced) caused \\n\\nstatistically significant differences in their experience. In \\n\\nparticular, those who regarded themselves as having \\n\\nadvanced familiarity with the videos found interactions \\n\\nmore interesting (mean: 4.5) than those who regarded \\n\\nthemselves as beginners (mean: 3.7) at p > 0.05, t=2.37, \\n\\ndegrees of freedom 18, standard deviation 0.770. Those \\n\\nwith advanced familiarity also contrasted with participant \\n\\nwho considered themselves in the middle, in that the \\n\\ninteractions were happy (mean: 4.4 vs. 3.6) at p < 0.05 \\ndescriptive \\nfeature \\naverage\\n min\\n max enjoyable\\n 4.1 1 5 friendly\\n 3.0 1 5 impressive\\n 3.7 1 5 nice\\n 4.1 2 5 interesting\\n 4.1 2 5 relaxed\\n 3.6 1 5 anxious\\n 2.3 1 5 natural\\n 3.4 1 5 happy 4.2 2 5 tense\\n 2.0 1 4 awkward\\n 1.9 1 5 angry 1.0 1 2 Average\\n 3.1   Figure 3 No significant difference between gender impressions except for interesting (p < 0.01) \\nFigure 4 Significant differences between advanced and beginners on interesting, and betwe\\nen advanced and middle on \\nhappy (p < 0.05).\\n 2766(t=2.27, degrees of freedom 32, standard deviation 0.786).  \\nBig deviations were also found in that the advanced \\n\\nparticipants found interactions more awkward, anxious, \\n\\nand tense than the middle-level participants, and they \\n\\nconsidered interaction more tense, yet nicer than the \\n\\nbeginners.  \\n\\nAs said, not all differences are significant. However, what \\n\\nis significant is the contrast between participants who \\n\\nevaluate themselves as experts vs. not experts on videos \\n\\nand video recordings. Apparently the participants™ interest \\n\\nand knowledge of the novel technology also carries over \\n\\nto situations where the novel technology is used, and thus \\n\\nalso affects their experience of the communication in that \\n\\nparticular situation. It is good to remember that the \\n\\nparticipants™ knowledge of computers and information \\n\\ntechnology as such did not cause a similar effect in the \\n\\nparticipant™s experience. This supports the hypothesis that \\n\\nnew expertise or knowledge which deviates from the \\n\\nusual knowledge in conversational situations tends to have \\n\\na positive effect on the evaluation and experience of the \\n\\nsituation as a whole. \\n5. Engagement \\nConversations are cooperative activities through which \\n\\nthe participants aim at achieving some underlying goals \\n\\nof the interaction. The goals can range from specific \\n\\ntask-related goals (e.g. to instruct someone to use \\n\\nannotation software, to provide information about bus \\n\\ntime, to learn to know each other) to an intention just to \\n\\nkeep the ﬁchannel openﬂ.  The interlocutors react to each \\n\\nothers™ actions and coordinate their turns in a manner \\n\\nthat allows both to present their message in a cooperative \\n\\nmanner. Engagement is an important sign of this kind of \\n\\ncooperation, and it is related to the interlocutors™ \\n\\nexperience of the interaction in general: more engaged \\n\\nthe interlocutors are in the conversation, the more \\n\\npositively they may experience the interaction. \\n\\nOur interest in studying conversational engagement goes \\n\\nback to intelligent systems and interaction technology, \\n\\nwhere engagement is used to describe the user™s \\n\\nwillingness and involvement in the interaction with the \\n\\nautomatic interactive system. If it is possible to measure \\n\\nthe interlocutors™ engagement level, it is easier to adjust \\n\\nthe system™s conversational strategies accordingly.  \\n\\nAn intuitive definition of engagement is that it refers to \\n\\nthe situation where the participants are involved in a \\n\\nconversation and show basic willingness to listen to the \\n\\npartner and provide coherent contributions. However, the \\n\\nmore active the interlocutors become, the more engaged \\n\\nthey seem to be in the conversation: their speaking \\n\\nfrequency, tone of voice and body posture indicate \\n\\ninterest and commitment to the topic of the conversation: \\n\\nthey engagement becomes embodied. Usually such \\n\\nactivity is reinforced by the partner™s actions on a similar \\n\\nlevel of engagement, so we can talk about mutual \\n\\nengagement. Such interactions are described as pleasant, \\n\\ninspiring, and fun.  \\n\\nWe will investigate various action patterns and \\n\\nbehaviours that are typical for the participants when they \\nare engaged in interaction they describe as nice, \\n\\nenjoyable, interesting, natural, pleasant.  We will explore \\n\\nsome measures of engagement in terms of the \\n\\ninterlocutors™ verbal and non-verbal communicative \\n\\nactivity, in particular their gesture activity and study if \\n\\nengagement can be operationalised through these signals. \\n\\nPrevious work has used such measures as frequency \\n\\ncounts and the utterance density (Campbell and Scherer, \\n\\n2010; Jokinen, 2011), and we will follow these lines as is \\n\\nappropriate.  \\n\\nWe also emphasise the close relationship between speech \\n\\nand multimodal information in the processing of human \\n\\nconversational interactions. The semantic content of \\n\\nlinguistic utterances is accompanied by hand gestures, \\n\\nbody movement, eye-gaze, and non-speech vocalisations \\n\\nwhich are used as tacit signals to indicate the speaker's \\n\\nfocus of attention, intentions, emphasis, emotional state, \\n\\netc. They should, of course, be processed alongside the \\n\\nlanguage expressions.  \\n\\nMoreover, the participants™ synchrony with each other is \\n\\nregarded as one of the pertinent signs of cooperation: the \\n\\ninterlocutors intuitively tend to follow the partner™s \\n\\ncommunication and produce similar behaviour, thus \\n\\ncontributing to the construction of the shared context and \\n\\nmutual understanding. This kind of adaptation to each \\n\\nother™s behaviour is often called alignment (Pickering \\n\\nand Garrod, 2004), or mimicry, and it can take place \\n\\nverbally (words, prosody) or non-verbally (gestures, \\n\\nbody posture). It is thus an important sign of the partners™ \\n\\nengagement in the interaction, and our previous works \\n\\non this can be found in Jokinen and Pärkson (2011) as \\n\\nwell as in Rummo and Tenjes (2011).  \\n\\nWe also have data and first results about specific type of \\n\\ncommunication Œ subject with the Patau syndrome. The \\n\\npreliminary results of the analysis of data (see Jokinen et \\n\\nal. forthcoming) reveal meaningful nonverbal behaviour \\n\\nthrough touching. For instance, the Patau subject put her \\n\\nhand on her partner™s shoulder, creating her own \\n\\ncommunicative space. From this act we can conclude \\n\\nthat \\ntouch\\n is clearly one component of Patau subject™s \\nlanguage. This type of nonverbal interactive behaviour \\n\\nguarantees to subject that her partner is involved in the \\n\\ninteraction as well as supplies her necessity of adjacency. \\n\\nWhen people engage in dialogue, they use verbal and \\n\\nnon-verbal cues to structure the conversation flow and \\n\\nprovide feedback about the current understanding of the \\n\\ndiscourse. An intuitive measure of cooperation between \\n\\ninterlocutors is their verbal and non-verbal \\n\\ncommunicative activity, and we hypothesize that \\n\\nengagement can be measured analogously, with respect \\n\\nto their cooperation. It can be estimated by measuring the \\n\\nparticipants™ verbal and non-verbal activity. In this we \\n\\nuse both quantitative and qualitative analysis; the latter \\n\\ngives an opportunity to show variety of the research \\n\\nmaterial. \\n\\nIn another previous study, we used annotated multiparty \\n\\nconversations where three participants (who are familiar \\n\\nwith each other) had been assigned certain roles related \\n\\nto a simulated school inspection situation. The analysis \\n2767shows that participants use a full repertoire of different \\nnon-verbal signals as signs of their engagement in the \\n\\nconversation: hand gestures, facial displays, nods, and \\n\\nbody movement. For instance, the speakers animate and \\n\\nemphasise their speech by hand gestures, so as to give \\n\\nimportance to a particular part of their speech, and they \\n\\nalso use gestures to control and coordinate conversation \\n\\nflow. Also the speakers™ gaze is used as a pointing \\n\\ngesture: it can mark the speaker who is expected to take \\n\\nthe next turn (mutual gaze). It is also related to the \\n\\nprocessing of the given information and is effectively \\n\\nused to create social bonds between the interlocutors. \\n\\nSome body movements are also used to fill pauses in \\n\\nconversation: if the speaker does not want to take the \\n\\nturn or is unable to take the turn, they usually withdraw \\n\\nfrom the centre of the conversational space. By body \\n\\nmovement, the participants tacitly indicate that they are \\n\\npresent in the conversation.  \\n\\nOther communicative signals include nodding and \\n\\nlaughing. Nodding is related to the interlocutor™s \\n\\nengagement because the very act of nodding signals that \\n\\nthe person takes part in a conversation and is ready for \\n\\ncooperation. Laughing often occurs in smooth \\n\\nconversations at the same time. On other hand, laughing \\n\\ncan also illustrates the case where engagement is \\n\\nunevenly distributed among the interlocutors: not all are \\n\\nengaged to the same level at the same time. \\n\\nThe fresh First Encounters corpus will provide valuable \\n\\nreference material for these earlier corpora and related \\n\\nstudies. Moreover, it may help us also to shed light on \\n\\nthe general question of whether the participants™ \\n\\nnonverbal behaviour differs in role-playing and \\n\\nspontaneous situations, and if so what are the \\n\\ncharacteristics.  \\n6. Conclusions and Future Work \\nSeveral interesting issues emerged from the corpus \\n\\ncollection and preliminary data analyses, which can lead \\n\\nto subsequent research. For instance, we have studied the \\n\\ninterlocutors™ self-evaluation of the interaction they were \\n\\nengaged in, and although the dataset is relatively small, \\n\\nwe found statistically significant differences concerning \\n\\nhow the participants experienced the interaction. \\n\\nInterestingly, the participants™ expertise on the novel \\n\\ntechnology that was used in the data collection seems to \\n\\nbe one of the distinguishing factors, indicating that \\n\\nengagement and experience are complex issues, related \\n\\nto intrigue aspects of individual skills and knowledge.  \\n\\nWe will also continue engagement studies and aim at \\n\\nbuilding models that would allow us to better understand \\n\\nthe complex process of cooperation and coordination of \\n\\ninteractions. For this, we will especially study gesturing \\n\\nand gesture recognition, and can use all the that has been \\n\\ncollected in various activities. \\n\\nThe research will also focus on intercultural comparison \\n\\nof the collected conversational data. Systematic \\n\\nmultimodal communication studies are useful, and they \\n\\ncan provide valuable empirical evidence for the \\n\\nobservations and views concerning different \\ncommunication styles and strategies in different cultures. \\n\\nEspecially in the Nordic context, comparisons between \\n\\nthe first encounter dialogues in the neighbouring countries \\n\\nwill be particularly interesting because the countries have \\n\\na long history of various interactions and encounters on \\n\\npolitical and cultural levels, while the languages are pair-\\n\\nwise linguistically related. It is also possible to continue \\n\\nthis kind of comparison by extending our research to \\n\\ncorpora that represent larger cultural differences, such as \\n\\nJapanese (Jokinen et al. 2010). \\n\\nWe are currently in the process of transcribing and \\n\\nannotating the First Encounters Data. As future work, we \\n\\nplan to collect more data, and may especially focus on \\n\\nmulti-party conversations where the participants have \\n\\ndifferent roles: the speaker, the main addressee, and the \\n\\nside addressee(s). In these situations the participants™ role \\n\\nis important and their level of engagement can differ \\n\\ndepending on whether the participant is actively engaged \\n\\nin the interaction or listening to a conversation as a side \\n\\nparticipant. For instance, the side participant need not \\n\\nreact at the same time as the main addressee, and still be \\n\\nengaged in the conversation. \\n7. Acknowledgements \\nWe would like to thank the participants who took part in \\nthe video recordings and Sven Laater who took care of \\n\\nthe practical video setup. We would also like to thank \\nMare Koit and her project in the Estonian Centre of \\nExcellence in Computer Science (EXCS) for \\n\\nencouragement and financial support for the work. The \\n\\nMINT project is financially supported by the Estonian \\n\\nScience Foundation grant ETF8958. \\n8. References Allwood, J., Cerrato, L., Jokinen, K., Navarretta, C. and \\nPaggio, P. (2007) The MUMIN Coding Scheme for the \\n\\nAnnotation of Feedback, Turn Management and \\n\\nSequencing. In Martin et al. (eds) \\nMultimodal \\nCorpora for Modelling Human Multimodal Behaviour\\n.  Argyle, M. (1975) Bodily Communication. Methuen & \\nCo. Ltd. London.  \\nBoersma, P. and D. Weenink (2009). Praat: doing \\nphonetics by computer (version 5.1.05). Retrieved \\n\\nMay 1, 2009, from http://www.praat.org/   \\nCampbell, N., Scherer, S. 2010. Comparing Measures of \\nSynchrony and Alignment in Dialogue Speech Timing \\n\\nwith respect to Turn-taking Activity. \\nProceedings of \\nInterspeech\\n. Makuhari, Japan \\nJokinen, K. (2011). Turn taking, Utterance Density, and \\nGaze Patterns as Cues to Conversational Activity. \\nICMI-MMI 2011\\n, November 14-18, 2011, Alicante, \\nSpain. \\nJokinen, K., Tenjes, S., Rummo, I. forthcoming. \\nEmbodied interaction and semiotic categorization: \\ncommunicative gestures of a girl with Patau \\n\\nsyndrome. In C. Paradis et al. (eds) \\nThe Construal of \\nSpatial Meaning: Windows into Conceptual Space\\n. Oxford: Oxford University Press, 38 pp. \\nJokinen, K. and S. Pärkson, 2011. Synchrony and \\nCopying in Conversational Interactions. The 3\\nrd Nordic Symposium on Multimodal Interaction, \\n\\nHelsinki, May 2011. \\n2768Jokinen, K., Nishida, M., Yamamoto, S. 2010. Collecting \\nand Annotating Conversational Eye-Gaze Data. \\nProceedings of Workshop ﬁMultimodal \\n\\nCorpora:Advances in Capturing, Coding and \\n\\nAnalyzing Multimodality\\nﬂ, Language Resources and \\nEvaluation Conference (LREC), Malta. \\nNavarretta, C., E. Ahlsén, J. Allwood, K. Jokinen, P. \\nPaggio (2011) Creating Comparable Multimodal \\n\\nCorpora for Nordic Languages. In \\nProceedings of the \\n18th Nodalida\\n., 153-160. \\nNezlek, J. B. (2010). Multilevel modeling and cross-\\ncultural research. In D. Matsumoto and A. J. R. van de \\nVijver (Eds.) \\nCross-Cultural research methods in \\npsychology\\n. Oxford. \\nPaggio, P., J. Allwood, E. Ahlsén, K. Jokinen, C. \\nNavarretta (2010). The NOMCO multimodal Nordic \\nresource - goals and characteristics. In \\nProceedings of \\nLREC 2010\\n, 2968-2973. \\nPickering, M, Garrod, S. 2004. Towards a mechanistic \\npsychology of dialogue, Behavioral and Brain \\n\\nSciences 27, 169Œ 226. \\nRehm, M., E. André, N. Bee, B. Endrass, M. Wissner, Y. \\nNakamo, A. Akhter Lipi, T. Nishida and H.H. Huang \\n\\n(2009). The Intercultural Dimension of Multimodal \\nCorpora. In Kipp et al. (eds) \\nMultimodal Corpora\\n. LNAI 5509. Springer,138Œ159.  \\nRummo, I. and S.Tenjes (2011). AJA mõistestamine \\nPatau \\nsündroomiga subjekti suhtluses. \\n(Conceptualization of TIME in the context of Patau \\nsyndrome). In H. Metslang et al. (eds) \\nEesti \\nRakenduslingvistika Ühingu aastaraamat 7 / Estonian \\n\\nPapers in Applied Linguistics 7\\n. Tallinn: Eesti Keele \\nSihtasutus, pp 231-247. \\nWilcock, G. and K. Jokinen (2011). Adding Speech to a \\nRobotics Simulator. \\nProceedings of the Paralinguistic \\nInformation and its Integration in Spoken Dialogue \\nSystems Workshop (IWSDS 2011)\\n, Granada, Spain, pp \\n371-376. \\n 2769\""
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
